{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from gzip import open as gopen\n",
    "import subprocess\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "# seed the rng\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each simulation directory to generate corrected transmission networks, sample data and time trees\n",
    "for i in range(1,1101):\n",
    "    # generate latent period\n",
    "    latent_period = np.random.exponential(2.9/365) #expected value is 2.9/365 days\n",
    "    # set up file paths for transmission network\n",
    "    old_tn_path = f'simulations/{i:04d}/transmission_network.subsampled.txt.gz'\n",
    "    new_tn_path = f'simulations/{i:04d}/transmission_network.subsampled.corrected.txt'\n",
    "    # open files and write out the corrected data\n",
    "    with gopen(old_tn_path, 'rt') as old_tn, open(new_tn_path, 'w') as new_tn:\n",
    "        for line in old_tn:\n",
    "            u, v, t = line.strip().split('\\t')\n",
    "            new_tn.write(f'{u}\\t{v}\\t{float(t) + latent_period:.6f}\\n')\n",
    "    # set up file paths for sample times\n",
    "    old_samples_path = f'simulations/{i:04d}/subsample_times.txt.gz'\n",
    "    new_samples_path = f'simulations/{i:04d}/subsample_times.corrected.txt'\n",
    "    # open files and write out the corrected data\n",
    "    with gopen(old_samples_path, 'rt') as old_samples, open(new_samples_path, 'w') as new_samples:\n",
    "        for line in old_samples:\n",
    "            u, t = line.strip().split('\\t')\n",
    "            new_samples.write(f'{u}\\t{float(t) + latent_period}\\n')\n",
    "    # setup and run CoaTRan to generate corrected time trees\n",
    "    command = ['coatran_constant', new_tn_path, new_samples_path, '1']\n",
    "    env = os.environ.copy()\n",
    "    coatran_rng_seed = np.random.randint(low=0, high=2**31) # ensure it is reproducible\n",
    "    env[\"COATRAN_RNG_SEED\"] = str(coatran_rng_seed)\n",
    "    result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Error executing CoaTran for simulation {i}: {result.stderr.decode()}\")\n",
    "        print(command)\n",
    "    else:\n",
    "        tree_path = f'simulations/{i:04d}/tree.time.subsampled.corrected.nwk'\n",
    "        with open(tree_path, 'w') as file_handle:\n",
    "            file_handle.write(result.stdout.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup output folder for each of 1000 resamples\n",
    "# each with subfolders for CC and AB clade analysisiresults\n",
    "if os.path.exists('resamples'):\n",
    "    # Remove the directory and its contents\n",
    "    shutil.rmtree('resamples')\n",
    "os.makedirs('resamples')\n",
    "for i in range(1000):\n",
    "    # create path for this resample\n",
    "    os.makedirs(f'resamples/{i}')\n",
    "    # create subdirectory with clade analysis results subdirectories\n",
    "    os.makedirs(f'resamples/{i}/clade_analyses_CC')\n",
    "    os.makedirs(f'resamples/{i}/clade_analyses_AB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to call stableCoalescence_cladeAnalysis.py\n",
    "# to resample one simulation 1000 times\n",
    "def resample_simulation(i, seed):\n",
    "    command = [\n",
    "        \"python3\",\n",
    "        \"stableCoalescence_cladeAnalysis.py\",\n",
    "        \"-tn\", f'simulations/{i:04d}/transmission_network.subsampled.corrected.txt',\n",
    "        \"-tt\", f'simulations/{i:04d}/tree.time.subsampled.corrected.nwk',\n",
    "        \"-g\", f'simulations/{i:04d}/GEMF_files/output.txt.gz',\n",
    "        \"-s\", \"0.00092\",\n",
    "        \"-m\", \"1\",\n",
    "        \"-id\", f'{i:04d}',\n",
    "        \"-seed\", str(seed)\n",
    "    ]\n",
    "    subprocess.run(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample each of the 1100 simulations with a deterministic seed\n",
    "seeds = np.random.randint(0, 2**31, size=1100)\n",
    "with ProcessPoolExecutor(max_workers=24) as executor:\n",
    "    executor.map(resample_simulation, range(1, 1101), seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary analysis (3.47-day doubling time, 15% ascertainment rate) tree sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clade_results(dir):\n",
    "    clade_analyses_d = dict()\n",
    "    for path in sorted(os.listdir(dir)): # pool all clade analyses together\n",
    "        clade_analysis_path = dir + path\n",
    "        key = int(path.split('_')[0])\n",
    "        clade_analyses_d[key] = {'clade_sizes': [], 'subclade_sizes': []}\n",
    "        for line in open(clade_analysis_path):\n",
    "            l = line.strip().strip(']').split('[')\n",
    "            clade_size = int(l[0].strip()) # make each clade size (including single leaves) an integer\n",
    "            subclade_sizes = [int(x) for x in l[1].strip().replace(' ', '').split(',')] # put each subclade size (including single leaves) into a list\n",
    "            clade_analyses_d[key]['clade_sizes'].append(clade_size)\n",
    "            clade_analyses_d[key]['subclade_sizes'].append(subclade_sizes)\n",
    "    return clade_analyses_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Less than 787 taxa: 0.017273\n",
      "More than 1000 taxa: 0.980909\n",
      "More than 5000 taxa: 0.953636\n"
     ]
    }
   ],
   "source": [
    "clade_analyses_CC_dir = './resamples/0/clade_analyses_CC/'\n",
    "clade_analyses_AB_dir = './resamples/0/clade_analyses_AB/'\n",
    "\n",
    "clade_analyses_CC_d = read_clade_results(clade_analyses_CC_dir)\n",
    "\n",
    "print('Less than 787 taxa: %f' % (sum([sum(clade_analyses_CC_d[x]['clade_sizes']) < 787 for x in clade_analyses_CC_d])/1100)) \n",
    "print('More than 1000 taxa: %f' % (sum([sum(clade_analyses_CC_d[x]['clade_sizes']) > 1000 for x in clade_analyses_CC_d])/1100))\n",
    "print('More than 5000 taxa: %f' % (sum([sum(clade_analyses_CC_d[x]['clade_sizes']) > 5000 for x in clade_analyses_CC_d])/1100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree shapes and bayes factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconstrained_results = np.array([1.68,80.85, 10.32, 0.92])/100 # linB, linA, C/C, T/T\n",
    "recCA_results = np.array([77.28, 8.18, 10.49, 3.71])/100 # linB, linA, C/C, T/T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bf(asr_results, simulation_results):\n",
    "    # Let t_p be a polytomy, t_1C be one clade, and t_2c be two clades. Note that t_p is a topology with a polytomy but is not t_1c. t_p + t_1c equals all topologies with a basal polytomy (Fig. 2a). \n",
    "    # trees are in the order\n",
    "    # (t_p, t_1C, t_2C, (t_p,(t_p,t_1C,t_2C)), (t_1C,(t_p,t_1C,t_2C)), (t_2c,(t_p,t_1C,t_2C)))\n",
    "    compatibility_matrix = np.array([np.array([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0]), # S_A\n",
    "                                     np.array([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0]), # S_B\n",
    "                                     np.array([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0]), # S_CC\n",
    "                                     np.array([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0])] # S_TT\n",
    "                                    )\n",
    "                                    \n",
    "    # A matrix of conditional probabilities\n",
    "    # Each row stores the vector Pr(S_MRCA | \\btau)\n",
    "    pr_s_mrca_given_tree = np.array([x/sum(x) if sum(x) > 0 else x for x in compatibility_matrix.T]) # if tree not associated with any haplotype, just keep the row as all 0s\n",
    "    \n",
    "    # Order: S_A, S_B, S_{C/C}, S_{T/T}\n",
    "    pr_s_mrca_given_data = np.array(asr_results)/sum(asr_results)\n",
    "    unnormalized_pr_data_given_s_mrca = pr_s_mrca_given_data.copy()\n",
    "    \n",
    "    # FAVITES simulation information\n",
    "    # the 3 trees are in the order (t_p, t_1C, t_2C)\n",
    "    pr_3_topos = np.array(simulation_results)\n",
    "    pr_trees_given_I1 = np.concatenate([pr_3_topos, np.array([0]*9)])\n",
    "    pr_trees_given_I2 = np.concatenate([np.array([0]*3), np.outer(pr_3_topos, pr_3_topos).flatten()])\n",
    "    \n",
    "    # Equal prior probability of 1 or 2 intros\n",
    "    pr_I1 = 0.5\n",
    "    pr_I2 = 0.5\n",
    "    \n",
    "    pr_s_mrca_and_I1 = np.dot([np.dot(pr_s_mrca_given_tree.T[i], pr_trees_given_I1) for i in range(0,4)], pr_I1) # dot product of P(haplotype|tree) (column) and P(trees|I_n), scaled by P(I_n)\n",
    "    pr_s_mrca_and_I2 = np.dot([np.dot(pr_s_mrca_given_tree.T[i], pr_trees_given_I2) for i in range(0,4)], pr_I2)\n",
    "\n",
    "    posterior_odds = np.dot(unnormalized_pr_data_given_s_mrca, pr_s_mrca_and_I2) / np.dot(unnormalized_pr_data_given_s_mrca, pr_s_mrca_and_I1)\n",
    "    prior_odds = pr_I2/pr_I1\n",
    "    BF = posterior_odds/prior_odds\n",
    "\n",
    "    return(BF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clade_analysis_updated(clade_analyses_CC_dir, clade_analyses_AB_dir, label, min_polytomy_size=100, _print_=False):\n",
    "    # C/C analyses focus on 1-mutation clades\n",
    "    # therefore, all descendant lineages are included:\n",
    "    #   -1 = unmutated leaf\n",
    "    #    1 = mutated leaf separate from a clade\n",
    "    #   >1 = clade\n",
    "    clade_analyses_CC_d = read_clade_results(clade_analyses_CC_dir)\n",
    "\n",
    "    # A/B analyses focus on 2-mutation clades\n",
    "    # only 2-mutation clades are included \n",
    "    clade_analyses_AB_d = read_clade_results(clade_analyses_AB_dir)\n",
    "    \n",
    "    # C/C analysis\n",
    "    cc_count_30perc_twoPolytomies = 0 # how often are there only two 1-mutation clades, each constituting more than 30% of taxa AND with a polytomy at the base of each clade, and no other descendants from the root?\n",
    "    for run in clade_analyses_CC_d:\n",
    "        clade_sizes = clade_analyses_CC_d[run]['clade_sizes'] # get the clade sizes\n",
    "        subclade_sizes = [len(x)>=min_polytomy_size for x in clade_analyses_CC_d[run]['subclade_sizes']] # check if each clade has a polytomy at the base\n",
    "        if len(clade_sizes) == 2: # if there are more than two descendants, including individual leaves, skip\n",
    "            if min(clade_sizes) > (sum(clade_sizes)*0.30): # make sure each clade is >30%\n",
    "                if False not in subclade_sizes: # each clade must have a polytomy at the base\n",
    "                    cc_count_30perc_twoPolytomies += 1\n",
    "    \n",
    "    # A/B analysis\n",
    "    ab_count_30perc_twoPolytomies = 0 # interested in 2 mutations clade that are at least 30% of all taxa + has a basal polytomy + polytomy at 2 mutation clade\n",
    "    lower_constraint = 0.3 # the 2-mutation clade must be at least 30% of all taxa\n",
    "    upper_constraint = 0.7 # the 2-mutation clade must be at most 70% of all taxa\n",
    "\n",
    "    for run in clade_analyses_AB_d:\n",
    "        num_leaves = sum(clade_analyses_CC_d[run]['clade_sizes'])\n",
    "        base_polytomy_size = len(clade_analyses_CC_d[run]['clade_sizes']) # check how many lineages descend from the root\n",
    "        clade_sizes = clade_analyses_AB_d[run]['clade_sizes'] \n",
    "        subclade_sizes = clade_analyses_AB_d[run]['subclade_sizes']\n",
    "        if not clade_sizes: # no 2 mutation clades\n",
    "            continue\n",
    "        for index, clade_size in enumerate(clade_sizes): # loop through all 2 mutation clades\n",
    "            if lower_constraint*num_leaves <= clade_size <= upper_constraint*num_leaves: # clade match size restrictions\n",
    "                if base_polytomy_size >= min_polytomy_size: # basal polytomy\n",
    "                    if len(subclade_sizes[index]) >= min_polytomy_size: # polytomy at 2 mutation clade\n",
    "                        ab_count_30perc_twoPolytomies += 1 # if all conditions are met, add 1 to the count\n",
    "                        break # if one 2 mutation clade meets the conditions, break out of the loop and move on to the next run\n",
    "\n",
    "    # polytomy: checking if there is a polytomy at the base of the tree; can do this using the C/C analysis\n",
    "    min_polytomy_descendants = min_polytomy_size\n",
    "    count_atLeastMinDescendants = 0\n",
    "    for run in clade_analyses_CC_d: # loop through all runs\n",
    "        clade_sizes = clade_analyses_CC_d[run]['clade_sizes'] # get the clade sizes\n",
    "        if len(clade_sizes) >= min_polytomy_descendants:  # if there are at least X number of descendants, including individual leaves\n",
    "            count_atLeastMinDescendants += 1\n",
    "    polytomy_result = count_atLeastMinDescendants/1100\n",
    "    \n",
    "    # calculate bayes factors\n",
    "    cc_result = cc_count_30perc_twoPolytomies/1100\n",
    "    ab_result = ab_count_30perc_twoPolytomies/1100\n",
    "\n",
    "    simulation_results = [polytomy_result-ab_result, ab_result, cc_result] # note that polytomy result is inclusive of ab_result, so subtract ab_result from polytomy_result to avoid double counting\n",
    "    unconstrained_results = np.array([1.68, 80.85, 10.32, 0.92])/100 # linA, linB, C/C, T/T\n",
    "    recCA_results = np.array([77.28, 8.18, 10.49, 3.71])/100 # linA, linB, C/C, T/T\n",
    "    bf_unconstrained = calculate_bf(unconstrained_results, simulation_results)\n",
    "    bf_recCA = calculate_bf(recCA_results, simulation_results)\n",
    "    \n",
    "    if _print_ == True:\n",
    "        print('Proportion with a polytomy at the base: %f\\n' % (count_atLeastMinDescendants/1100))\n",
    "        print('C/C clade analysis: 2 clades, polytomy at base of each clade, each clade possessing at least 30%% of the taxa: %f\\n' % (cc_count_30perc_twoPolytomies/1100))\n",
    "        print('A/B clade analysis: Basal polytomy, polytomy at 2-mut clade, with the clade possessing between 30 and 70%% of the taxa: %f\\n' % (ab_count_30perc_twoPolytomies/1100))\n",
    "        print('Unconstrained Bayes factor: %f' % bf_unconstrained)\n",
    "        print('recCA Bayes factor: %f' % bf_recCA)\n",
    "\n",
    "    return [cc_count_30perc_twoPolytomies, ab_count_30perc_twoPolytomies, count_atLeastMinDescendants, bf_unconstrained, bf_recCA]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect clade analysis results\n",
    "def process_clade_analysis(i):\n",
    "    clade_analyses_CC_dir = f'./resamples/{i}/clade_analyses_CC/'\n",
    "    clade_analyses_AB_dir = f'./resamples/{i}/clade_analyses_AB/'\n",
    "    return clade_analysis_updated(clade_analyses_CC_dir, clade_analyses_AB_dir, '3.5 DT', min_polytomy_size=100)\n",
    "\n",
    "# Initialize lists to store results\n",
    "cc_counts = []\n",
    "ab_counts = []\n",
    "polytomy_counts = []\n",
    "bf_unconstrained_results = []\n",
    "bf_recCA_results = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=24) as executor:\n",
    "    # Map process_clade_analysis function across the range of values\n",
    "    results = executor.map(process_clade_analysis, range(1000))\n",
    "\n",
    "    # Unpack the results and append them to the respective lists\n",
    "    for a, b, c, d, e in results:\n",
    "        cc_counts.append(a)\n",
    "        ab_counts.append(b)\n",
    "        polytomy_counts.append(c)\n",
    "        bf_unconstrained_results.append(d)\n",
    "        bf_recCA_results.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Analysis</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Frequency</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Bayes Factor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Doubling Time (days)</th>\n",
       "      <th>Ascertainment Rate</th>\n",
       "      <th>Basal Polytomy (%)</th>\n",
       "      <th>CC Topology (%)</th>\n",
       "      <th>AB Topology (%)</th>\n",
       "      <th>unconstrained</th>\n",
       "      <th>recCA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.47</td>\n",
       "      <td>0.15</td>\n",
       "      <td>44.32</td>\n",
       "      <td>0.17</td>\n",
       "      <td>3.34</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Analysis                             Frequency                  \\\n",
       "  Doubling Time (days) Ascertainment Rate Basal Polytomy (%) CC Topology (%)   \n",
       "0                 3.47               0.15              44.32            0.17   \n",
       "\n",
       "                   Bayes Factor        \n",
       "  AB Topology (%) unconstrained recCA  \n",
       "0            3.34           3.4   3.5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write out means\n",
    "# Calculate mean counts\n",
    "mean_cc_count = np.mean(cc_counts)\n",
    "mean_ab_count = np.mean(ab_counts)\n",
    "mean_polytomy_count = np.mean(polytomy_counts)\n",
    "\n",
    "# Convert means to topology likelihoods for the Bayes factor calculation\n",
    "mean_simulation_results = np.array([mean_polytomy_count - mean_ab_count, mean_ab_count, mean_cc_count]) / 1100\n",
    "\n",
    "# Setup the MRCA haplotype posterior probabilities for the Bayes factor calculation\n",
    "unconstrained_results = np.array([1.68, 80.85, 10.32, 0.92])/100  \n",
    "recCA_results = np.array([77.28, 8.18, 10.49, 3.71])/100  \n",
    "\n",
    "# Calculate Bayes factors of expected parameters\n",
    "bf_unconstrained = calculate_bf(unconstrained_results, mean_simulation_results)\n",
    "bf_recCA = calculate_bf(recCA_results, mean_simulation_results)\n",
    "\n",
    "# Calculate expected Bayes factors\n",
    "mean_unc = np.mean(bf_unconstrained_results)\n",
    "mean_recCA = np.mean(bf_recCA_results)\n",
    "\n",
    "with open('mean_resample_results_corrected.txt', 'w') as f:\n",
    "    f.write(f'expected CC rate: {mean_cc_count*100/1100:.2f}%\\n')\n",
    "    f.write(f'expected AB rate: {mean_ab_count*100/1100:.2f}%\\n')\n",
    "    f.write(f'expected polytomy rate: {mean_polytomy_count*100/1100:.2f}%\\n')\n",
    "    f.write(f'expected BF (unconstrained): {mean_unc:.1f}\\n')\n",
    "    f.write(f'expected BF (recCA): {mean_recCA:.1f}\\n')\n",
    "    f.write(f'BF of expected rates (unconstrained): {bf_unconstrained:.1f}\\n')\n",
    "    f.write(f'BF of exptected rates (recCA): {bf_recCA:.1f}\\n')\n",
    "    \n",
    "data = [\n",
    "    ['3.47','0.15',\n",
    "     f'{mean_polytomy_count*100/1100:.2f}',\n",
    "     f'{mean_cc_count*100/1100:.2f}', \n",
    "     f'{mean_ab_count*100/1100:.2f}', \n",
    "     f'{mean_unc:.1f}', \n",
    "     f'{mean_recCA:.1f}']\n",
    "]\n",
    "columns = pd.MultiIndex.from_tuples([\n",
    "    ('Analysis', 'Doubling Time (days)'),\n",
    "    ('Analysis', 'Ascertainment Rate'),\n",
    "    ('Frequency', 'Basal Polytomy (%)'),\n",
    "    ('Frequency', 'CC Topology (%)'),\n",
    "    ('Frequency', 'AB Topology (%)'),\n",
    "    ('Bayes Factor', 'unconstrained'),\n",
    "    ('Bayes Factor', 'recCA'),\n",
    "])\n",
    "df_multi_level = pd.DataFrame(data, columns=columns)\n",
    "df_multi_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
